"""
–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.
"""

import torch
from typing import Dict, List, Any, Tuple
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

from src.eval_lstm import evaluate_rouge
from src.configs import TOKENIZER
from src.utils.utils import preprocess_text
from .parameter_tuning import extract_rouge_score

def compare_models(lstm_model, baseline_model, val_loader: DataLoader, 
                  val_texts: List[str], device: torch.device, 
                  best_baseline_config: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏ —Å baseline –º–æ–¥–µ–ª—å—é.
    
    Args:
        lstm_model: –û–±—É—á–µ–Ω–Ω–∞—è LSTM –º–æ–¥–µ–ª—å.
        baseline_model: Baseline –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
        val_loader: DataLoader —Å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è LSTM.
        val_texts: –°–ø–∏—Å–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è baseline.
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.
        best_baseline_config: –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è baseline.
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
    """
    print("\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LSTM —Å baseline –º–æ–¥–µ–ª—å—é ===")
    
    # –û—Ü–µ–Ω–∫–∞ LSTM –º–æ–¥–µ–ª–∏
    print("–û—Ü–µ–Ω–∫–∞ LSTM –º–æ–¥–µ–ª–∏...")
    lstm_rouge_scores = evaluate_rouge(
        model=lstm_model,
        dataloader=val_loader,
        tokenizer=TOKENIZER,
        device=device
    )
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º LSTM ROUGE scores
    lstm_rouge1 = extract_rouge_score(lstm_rouge_scores['rouge1'])
    lstm_rouge2 = extract_rouge_score(lstm_rouge_scores['rouge2'])
    lstm_rouge_l = extract_rouge_score(lstm_rouge_scores['rougeL'])
    
    # –û—Ü–µ–Ω–∫–∞ baseline –º–æ–¥–µ–ª–∏
    print("–û—Ü–µ–Ω–∫–∞ baseline –º–æ–¥–µ–ª–∏...")
    if best_baseline_config:
        baseline_rouge_scores = baseline_model.evaluate_rouge(
            val_texts,
            max_samples=2000,
            max_new_tokens=20,
            do_sample=True,
            **{k: v for k, v in best_baseline_config.items() if k != 'name'}
        )
    else:
        baseline_rouge_scores = baseline_model.evaluate_rouge(
            val_texts,
            max_samples=2000,
            max_new_tokens=20
        )
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º baseline ROUGE scores
    baseline_rouge1 = extract_rouge_score(baseline_rouge_scores['rouge1'])
    baseline_rouge2 = extract_rouge_score(baseline_rouge_scores['rouge2'])
    baseline_rouge_l = extract_rouge_score(baseline_rouge_scores['rougeL'])
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    print(f"\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    print(f"{'–ú–æ–¥–µ–ª—å':<15} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10}")
    print("-" * 50)
    print(f"{'LSTM':<15} {lstm_rouge1:.4f}     {lstm_rouge2:.4f}     {lstm_rouge_l:.4f}")
    print(f"{'Baseline':<15} {baseline_rouge1:.4f}     {baseline_rouge2:.4f}     {baseline_rouge_l:.4f}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    if baseline_rouge_l > lstm_rouge_l:
        best_model = "Baseline"
        print(f"\nBaseline –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (ROUGE-L: {baseline_rouge_l:.4f} vs {lstm_rouge_l:.4f})")
    else:
        best_model = "LSTM"
        print(f"\nLSTM –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (ROUGE-L: {lstm_rouge_l:.4f} vs {baseline_rouge_l:.4f})")
    
    return {
        'lstm_rouge1': lstm_rouge1,
        'lstm_rouge2': lstm_rouge2,
        'lstm_rouge_l': lstm_rouge_l,
        'baseline_rouge1': baseline_rouge1,
        'baseline_rouge2': baseline_rouge2,
        'baseline_rouge_l': baseline_rouge_l,
        'best_model': best_model
    }

def generate_examples(lstm_model, baseline_model, tokenizer: AutoTokenizer, 
                     device: torch.device, sample_texts: List[str],
                     best_baseline_config: Dict[str, Any] = None) -> List[Dict[str, str]]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π.
    
    Args:
        lstm_model: LSTM –º–æ–¥–µ–ª—å.
        baseline_model: Baseline –º–æ–¥–µ–ª—å.
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.
        sample_texts: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        best_baseline_config: –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è baseline.
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
    """
    print("\n=== –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===")
    
    examples = []
    
    for i, text in enumerate(sample_texts):
        print(f"\n--- –ü—Ä–∏–º–µ—Ä {i+1} ---")
        print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text[:100]}...")
        
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            processed_text = preprocess_text(text, tokenizer=None)
            tokens = tokenizer.tokenize(processed_text)
            
            if len(tokens) < 4:
                print("–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue
                
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç (75%) –∏ —Ü–µ–ª—å (25%)
            cutoff = max(1, int(len(tokens) * 0.75))
            context_tokens = tokens[:cutoff]
            target_tokens = tokens[cutoff:]
            
            context_text = tokenizer.convert_tokens_to_string(context_tokens)
            target_text = tokenizer.convert_tokens_to_string(target_tokens)
            
            print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context_text}")
            print(f"–û–∂–∏–¥–∞–µ–º–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ: {target_text}")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å LSTM
            try:
                input_ids = tokenizer.encode(context_text, return_tensors="pt").to(device)
                lstm_generated = lstm_model.generate(
                    input_ids, tokenizer, 
                    max_new_tokens=len(target_tokens) + 5, 
                    top_k=20
                )
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
                lstm_continuation = lstm_generated[len(context_text):].strip()
                print(f"LSTM –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ: {lstm_continuation}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ LSTM: {e}")
                lstm_continuation = "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å baseline
            try:
                if best_baseline_config:
                    baseline_generated = baseline_model.generate(
                        context_text,
                        max_new_tokens=len(target_tokens) + 5,
                        **{k: v for k, v in best_baseline_config.items() if k != 'name'}
                    )
                else:
                    baseline_generated = baseline_model.generate(
                        context_text, 
                        max_new_tokens=len(target_tokens) + 5
                    ).strip()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
                baseline_continuation = baseline_generated[len(context_text):].strip()
                print(f"Baseline –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ: {baseline_continuation}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ Baseline: {e}")
                baseline_continuation = "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
            
            examples.append({
                'original': text,
                'context': context_text,
                'target': target_text,
                'lstm': lstm_continuation,
                'baseline': baseline_continuation
            })
            
        except Exception as e:
            print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏–º–µ—Ä–∞ {i+1}: {e}")
            print("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä")
            continue
    
    return examples

def create_comparison_report(results: Dict[str, Any], examples: List[Dict[str, str]]) -> str:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π.
    
    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
        examples: –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        
    Returns:
        –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç.
    """
    report = []
    report.append("=" * 60)
    report.append("ÔøΩÔøΩ –û–¢–ß–ï–¢ –û –°–†–ê–í–ù–ï–ù–ò–ò –ú–û–î–ï–õ–ï–ô –ê–í–¢–û–î–û–ü–û–õ–ù–ï–ù–ò–Ø –¢–ï–ö–°–¢–ê")
    report.append("=" * 60)
    
    report.append(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {results['best_model']}")
    
    report.append(f"\nÔøΩÔøΩ –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:")
    report.append(f"{'–ú–æ–¥–µ–ª—å':<15} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10}")
    report.append("-" * 50)
    report.append(f"{'LSTM':<15} {results['lstm_rouge1']:.4f}     {results['lstm_rouge2']:.4f}     {results['lstm_rouge_l']:.4f}")
    report.append(f"{'Baseline':<15} {results['baseline_rouge1']:.4f}     {results['baseline_rouge2']:.4f}     {results['baseline_rouge_l']:.4f}")
    
    report.append(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
    for i, example in enumerate(examples[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤
        report.append(f"\n--- –ü—Ä–∏–º–µ—Ä {i+1} ---")
        report.append(f"–ò—Å—Ö–æ–¥–Ω—ã–π: {example['original'][:50]}...")
        report.append(f"LSTM: {example['lstm']}")
        report.append(f"Baseline: {example['baseline']}")
    
    return "\n".join(report)